{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item31'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## get the data\n",
    "#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Define Global Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construct ImageDataGenerator Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Build, Compile and Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x1ca75de7c50>,\n",
       " <keras.layers.core.Dense at 0x1ca75e7e828>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the ResNet50 layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x1c9ec5c1908>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x1c9ec5c1a20>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9ec5c1128>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9e6005240>,\n",
       " <keras.layers.core.Activation at 0x1c9dc8d4358>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x1c9dc8e33c8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1c9e073b160>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9ec5cde80>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dc973588>,\n",
       " <keras.layers.core.Activation at 0x1c9dc973c18>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dc973d68>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dc973ef0>,\n",
       " <keras.layers.core.Activation at 0x1c9dc988d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dc9d3ba8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dc9ddb00>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dc9bcac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dca22898>,\n",
       " <keras.layers.merge.Add at 0x1c9dca3bc50>,\n",
       " <keras.layers.core.Activation at 0x1c9dca0cb70>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dca56f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dca80e10>,\n",
       " <keras.layers.core.Activation at 0x1c9dcaa2eb8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcaaf780>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcaafd30>,\n",
       " <keras.layers.core.Activation at 0x1c9dcac49e8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcaffd68>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcaff588>,\n",
       " <keras.layers.merge.Add at 0x1c9dcb13e10>,\n",
       " <keras.layers.core.Activation at 0x1c9dcb57dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcb575f8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcb72a90>,\n",
       " <keras.layers.core.Activation at 0x1c9dcaddb00>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcb96fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcba2160>,\n",
       " <keras.layers.core.Activation at 0x1c9dcba25c0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcbf0e48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcbf0940>,\n",
       " <keras.layers.merge.Add at 0x1c9dcc079e8>,\n",
       " <keras.layers.core.Activation at 0x1c9dcc3ee10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcc3eb38>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcc49a90>,\n",
       " <keras.layers.core.Activation at 0x1c9dcc6c780>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcc888d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcc9e160>,\n",
       " <keras.layers.core.Activation at 0x1c9dccb2be0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dccd7e48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcce4cf8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcce4b00>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcd2ff98>,\n",
       " <keras.layers.merge.Add at 0x1c9dcd459e8>,\n",
       " <keras.layers.core.Activation at 0x1c9dcd5f7f0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcd9cba8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcd88cf8>,\n",
       " <keras.layers.core.Activation at 0x1c9dcdae630>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcdca828>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcdca668>,\n",
       " <keras.layers.core.Activation at 0x1c9dcdf2c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dce18dd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dce30f98>,\n",
       " <keras.layers.merge.Add at 0x1c9dce4f320>,\n",
       " <keras.layers.core.Activation at 0x1c9dce18d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dce71f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dce7ea58>,\n",
       " <keras.layers.core.Activation at 0x1c9dce9e2b0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcebdb70>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcebdcc0>,\n",
       " <keras.layers.core.Activation at 0x1c9dced3e80>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcf0b828>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcf0bb38>,\n",
       " <keras.layers.merge.Add at 0x1c9dcf32c50>,\n",
       " <keras.layers.core.Activation at 0x1c9dcf57eb8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcf57ef0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcf70390>,\n",
       " <keras.layers.core.Activation at 0x1c9dcf62470>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9dcfb0748>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9dcfb0da0>,\n",
       " <keras.layers.core.Activation at 0x1c9dcfc5c88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9ddfccb00>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9ddfcc5f8>,\n",
       " <keras.layers.merge.Add at 0x1c9ddfe0e80>,\n",
       " <keras.layers.core.Activation at 0x1c9de019940>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de019dd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de043b38>,\n",
       " <keras.layers.core.Activation at 0x1c9de047b70>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de067f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de090fd0>,\n",
       " <keras.layers.core.Activation at 0x1c9de075470>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de0c1e80>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de0d6c88>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de0c19b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de10dd68>,\n",
       " <keras.layers.merge.Add at 0x1c9de126e80>,\n",
       " <keras.layers.core.Activation at 0x1c9de13cb38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de1579e8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de1829b0>,\n",
       " <keras.layers.core.Activation at 0x1c9de189a20>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de1a5b70>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de1bd400>,\n",
       " <keras.layers.core.Activation at 0x1c9de1b48d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de1c5a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de1ff6d8>,\n",
       " <keras.layers.merge.Add at 0x1c9de20c080>,\n",
       " <keras.layers.core.Activation at 0x1c9de24ce80>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de24ce10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de268eb8>,\n",
       " <keras.layers.core.Activation at 0x1c9de279550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de2be9b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de299588>,\n",
       " <keras.layers.core.Activation at 0x1c9de2b7f28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de2e3c18>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de2fb518>,\n",
       " <keras.layers.merge.Add at 0x1c9de2fb2b0>,\n",
       " <keras.layers.core.Activation at 0x1c9de333f28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9de31f1d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9de355b38>,\n",
       " <keras.layers.core.Activation at 0x1c9f2fec1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9f300cc50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9f300c550>,\n",
       " <keras.layers.core.Activation at 0x1c9f301fda0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9f32019b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9f30b7b38>,\n",
       " <keras.layers.merge.Add at 0x1c9f31f6f28>,\n",
       " <keras.layers.core.Activation at 0x1c9f3225ac8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9f3225eb8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9f323d160>,\n",
       " <keras.layers.core.Activation at 0x1c9f3273e48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9f3225c18>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9f327fcc0>,\n",
       " <keras.layers.core.Activation at 0x1c9f329ccf8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9f35faba8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1c9f35fa8d0>,\n",
       " <keras.layers.merge.Add at 0x1c9f3720da0>,\n",
       " <keras.layers.core.Activation at 0x1c9f3757d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1c9f37579b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca74a26a58>,\n",
       " <keras.layers.core.Activation at 0x1ca74a39a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca74a56c88>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca74a60cf8>,\n",
       " <keras.layers.core.Activation at 0x1ca74a60d30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca74ac4ba8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca74aae8d0>,\n",
       " <keras.layers.merge.Add at 0x1ca74acccf8>,\n",
       " <keras.layers.core.Activation at 0x1ca74afcb38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca74afcac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca74b08860>,\n",
       " <keras.layers.core.Activation at 0x1ca74b2a710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca74b49780>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca74b60f98>,\n",
       " <keras.layers.core.Activation at 0x1ca74b72d30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca74b94d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca74ba1c88>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca74ba1a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca74c05ba8>,\n",
       " <keras.layers.merge.Add at 0x1ca74c0ccf8>,\n",
       " <keras.layers.core.Activation at 0x1ca74c1b780>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca74c58e80>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca74c48c88>,\n",
       " <keras.layers.core.Activation at 0x1ca74c6b5c0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca74c92240>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca74c87908>,\n",
       " <keras.layers.core.Activation at 0x1ca74c9df60>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca75ca0f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca75cbcac8>,\n",
       " <keras.layers.merge.Add at 0x1ca75cbcf28>,\n",
       " <keras.layers.core.Activation at 0x1ca75d1af98>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca75cf0f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca75d12828>,\n",
       " <keras.layers.core.Activation at 0x1ca75d2b240>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca75d4b8d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca75d4bf60>,\n",
       " <keras.layers.core.Activation at 0x1ca75d64e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ca75da4390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1ca75d97da0>,\n",
       " <keras.layers.merge.Add at 0x1ca75db0f60>,\n",
       " <keras.layers.core.Activation at 0x1ca75de7c88>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x1ca75de7e48>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next we compile our model using the **adam** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "301/301 [==============================] - 457s 2s/step - loss: 0.0439 - accuracy: 0.9868 - val_loss: 0.0011 - val_accuracy: 0.9295\n",
      "Epoch 2/2\n",
      "301/301 [==============================] - 442s 1s/step - loss: 0.0140 - accuracy: 0.9965 - val_loss: 3.6597e-05 - val_accuracy: 0.9410\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
